{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Sentiment-Analysis-with-an-RNN\" data-toc-modified-id=\"Sentiment-Analysis-with-an-RNN-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Sentiment Analysis with an RNN</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Introduction</a></span></li></ul></li><li><span><a href=\"#Movie-Sentiment-Dataset-Downloading-to-a-Local-Folder\" data-toc-modified-id=\"Movie-Sentiment-Dataset-Downloading-to-a-Local-Folder-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Movie Sentiment Dataset Downloading to a Local Folder</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqHuBS09ydqC"
   },
   "source": [
    "# Sentiment Analysis with an RNN\n",
    "\n",
    "(code is adopted from https://github.com/PacktPublishing/Mastering-PyTorch/blob/master/Chapter04/rnn.ipynb and some texts from https://github.com/bentrevett/pytorch-sentiment-analysis)\n",
    "\n",
    "In this notebook, you'll implement a recurrent neural network that performs sentiment analysis.\n",
    "\n",
    "Using an RNN rather than a strictly feedforward network is more accurate since we can include information about the sequence of words.\n",
    "\n",
    "Here we'll use a dataset of movie reviews, accompanied by sentiment labels: positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "cBpEjJCQsQ6v",
    "outputId": "d61fcc39-11d8-4aac-de77-7f76931ffcf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f900d329d20>"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgxbLEIvUN_X"
   },
   "source": [
    "### Introduction\n",
    "\n",
    "We'll be using a **recurrent neural network** (RNN) as they are commonly used in analysing sequences. An RNN takes in sequence of words, $X=\\{x_1, ..., x_T\\}$, one at a time, and produces a _hidden state_, $h$, for each word. We use the RNN _recurrently_ by feeding in the current word $x_t$ as well as the hidden state from the previous word, $h_{t-1}$, to produce the next hidden state, $h_t$. \n",
    "\n",
    "$$h_t = \\text{RNN}(x_t, h_{t-1})$$\n",
    "\n",
    "Once we have our final hidden state, $h_T$, (from feeding in the last word in the sequence, $x_T$) we feed it through a linear layer, $f$, (also known as a fully connected layer), to receive our predicted sentiment, $\\hat{y} = f(h_T)$.\n",
    "\n",
    "Below shows an example sentence, with the RNN predicting zero, which indicates a negative sentiment. The RNN is shown in orange and the linear layer shown in silver. Note that we use the same RNN for every word, i.e. it has the same parameters. The initial hidden state, $h_0$, is a tensor initialized to all zeros. \n",
    "\n",
    "![picture](http://drive.google.com/uc?export=view&id=1n6yofIlp0edQ9Gieor00RhgOQDrdPro7)\n",
    "\n",
    "**Note:** some layers and steps have been omitted from the diagram, but these will be explained later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXAJbRZpsQ60"
   },
   "source": [
    "## Movie Sentiment Dataset Downloading to a Local Folder\n",
    "This is the IMDB Reviews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQWTiFrfsQ61",
    "outputId": "42b07ee4-352f-48f6-dcfd-0139a3007e96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-28 23:46:51--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  63.4MB/s    in 1.3s    \n",
      "\n",
      "2020-12-28 23:46:53 (63.4 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n",
      "Unzipping download file with reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:00<00:00, 45349.75it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 38609.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of reviews : 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "import tarfile\n",
    "\n",
    "print(\"Unzipping download file with reviews\")\n",
    "tar = tarfile.open(\"aclImdb_v1.tar.gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "# read sentiments and reviews data from the text files\n",
    "review_list = []\n",
    "label_list = []\n",
    "for label in ['pos', 'neg']:\n",
    "    for fname in tqdm(os.listdir(f'./aclImdb/train/{label}/')):\n",
    "        if 'txt' not in fname:\n",
    "            continue\n",
    "        with open(os.path.join(f'./aclImdb/train/{label}/', fname), encoding=\"utf8\") as f:\n",
    "            review_list += [f.read()]\n",
    "            label_list += [label]\n",
    "print ('\\nNumber of reviews :', len(review_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kAUiw_oysQ61",
    "outputId": "45f41101-0b48-4aa6-c81f-b9a024a7a946"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:01<00:00, 13232.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 334691), ('and', 162228), ('a', 161940), ('of', 145326), ('to', 135042), ('is', 106855), ('in', 93028), ('it', 77099), ('i', 75719), ('this', 75190)]\n"
     ]
    }
   ],
   "source": [
    "# pre-processing review text\n",
    "review_list = [review.lower() for review in review_list]\n",
    "review_list = [''.join([letter for letter in review if letter not in punctuation]) for review in tqdm(review_list)]\n",
    "\n",
    "# accumulate all review texts together\n",
    "reviews_blob = ' '.join(review_list)\n",
    "\n",
    "# generate list of all words of all reviews\n",
    "review_words = reviews_blob.split()\n",
    "\n",
    "# get the word counts\n",
    "count_words = Counter(review_words)\n",
    "\n",
    "# sort words as per counts (decreasing order)\n",
    "total_review_words = len(review_words)\n",
    "sorted_review_words = count_words.most_common(total_review_words)\n",
    "\n",
    "print(sorted_review_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cnh28zabsQ61",
    "outputId": "17303b0b-1dea-48fe-b4c0-67e0c4a7d7c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1), ('and', 2), ('a', 3), ('of', 4), ('to', 5), ('is', 6), ('in', 7), ('it', 8), ('i', 9), ('this', 10)]\n"
     ]
    }
   ],
   "source": [
    "# create word to integer (token) dictionary in order to encode text as numbers\n",
    "vocab_to_token = {word:idx+1 for idx, (word, count) in enumerate(sorted_review_words)}\n",
    "print(list(vocab_to_token.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sHQ0PSuksQ62",
    "outputId": "78e8297a-c7fd-4261-81d2-74648d317cd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i dont know what it is i find so endearing about this film but the first time i saw it i wanted to see how it ended im not a big fan of paul winfield nor of wardramas but i was truly wondering just how and when winfield would find his child all he knows is that the boy has green eyes truth be told i have not seen this movie in years nor has it been shown on tv in a while but this movie is somewhat of one mans odyssey after the pains of war winfield shows a very sympathetic and heart warming portrayal of a man lost by his memories there is an underlying message in this movie that he is looking for the last shred of human morality in the aftermath of this war and the reality that he does confront why this movie is not yet on dvd or video is a mystery to me\n",
      "\n",
      "[9, 89, 117, 48, 8, 6, 9, 159, 37, 3208, 42, 10, 19, 18, 1, 85, 61, 9, 207, 8, 9, 452, 5, 67, 86, 8, 1044, 141, 21, 3, 194, 329, 4, 711, 18053, 859, 4, 57550, 18, 9, 13, 352, 1450, 40, 86, 2, 50, 18053, 58, 159, 24, 512, 31, 26, 668, 6, 11, 1, 443, 43, 1421, 515, 871, 27, 565, 9, 25, 21, 107, 10, 17, 7, 147, 859, 43, 8, 74, 594, 20, 243, 7, 3, 130, 18, 10, 17, 6, 609, 4, 28, 1426, 6710, 100, 1, 7307, 4, 332, 18053, 264, 3, 52, 2231, 2, 522, 5248, 1111, 4, 3, 132, 410, 32, 24, 1846, 47, 6, 33, 4883, 734, 7, 10, 17, 11, 26, 6, 279, 15, 1, 225, 7619, 4, 400, 3614, 7, 1, 7008, 4, 10, 332, 2, 1, 638, 11, 26, 120, 6166, 133, 10, 17, 6, 21, 240, 20, 292, 41, 392, 6, 3, 754, 5, 69]\n"
     ]
    }
   ],
   "source": [
    "reviews_tokenized = []\n",
    "for review in review_list:\n",
    "    word_to_token = [vocab_to_token[word] for word in review.split()]\n",
    "    reviews_tokenized.append(word_to_token)\n",
    "print(review_list[0])\n",
    "print()\n",
    "print (reviews_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONd2MTclsQ62"
   },
   "outputs": [],
   "source": [
    "# encode sentiments as 0 or 1\n",
    "encoded_label_list = [1 if label =='pos' else 0 for label in label_list]\n",
    "\n",
    "reviews_len = [len(review) for review in reviews_tokenized]\n",
    "\n",
    "reviews_tokenized = [reviews_tokenized[i] for i, l in enumerate(reviews_len) if l>0 ]\n",
    "encoded_label_list = np.array([encoded_label_list[i] for i, l in enumerate(reviews_len) if l> 0 ], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "ZLDNOivUsQ62",
    "outputId": "7e29d8f0-b854-42bf-ed8d-79e80e649e42"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pad_sequence(reviews_tokenized, sequence_length):\n",
    "    ''' returns the tokenized review sequences padded with 0's or truncated to the sequence_length.\n",
    "    '''\n",
    "    padded_reviews = np.zeros((len(reviews_tokenized), sequence_length), dtype = int)\n",
    "    \n",
    "    for idx, review in enumerate(reviews_tokenized):\n",
    "        review_len = len(review)\n",
    "        \n",
    "        if review_len <= sequence_length:\n",
    "            zeroes = list(np.zeros(sequence_length-review_len))\n",
    "            new_sequence = zeroes+review\n",
    "        elif review_len > sequence_length:\n",
    "            new_sequence = review[0:sequence_length]\n",
    "        \n",
    "        padded_reviews[idx,:] = np.array(new_sequence)\n",
    "    \n",
    "    return padded_reviews\n",
    "\n",
    "sequence_length = 512\n",
    "padded_reviews = pad_sequence(reviews_tokenized=reviews_tokenized, sequence_length=sequence_length)\n",
    "\n",
    "plt.hist(reviews_len);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_wm8eeZsQ63"
   },
   "outputs": [],
   "source": [
    "train_val_split = 0.75\n",
    "train_X = padded_reviews[:int(train_val_split*len(padded_reviews))]\n",
    "train_y = encoded_label_list[:int(train_val_split*len(padded_reviews))]\n",
    "validation_X = padded_reviews[int(train_val_split*len(padded_reviews)):]\n",
    "validation_y = encoded_label_list[int(train_val_split*len(padded_reviews)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NWV9_5vsQ63"
   },
   "outputs": [],
   "source": [
    "## If while training, you get a runtime error that says: \"RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long\".\n",
    "## simply uncomment run the following lines of code additionally\n",
    "# train_X = train_X.astype('int64')\n",
    "# train_y = train_y.astype('int64')\n",
    "# validation_X = validation_X.astype('int64')\n",
    "# validation_y = validation_y.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlT1LwJwsQ63"
   },
   "outputs": [],
   "source": [
    "# generate torch datasets\n",
    "train_dataset = TensorDataset(torch.from_numpy(train_X).to(device), torch.from_numpy(train_y).to(device))\n",
    "validation_dataset = TensorDataset(torch.from_numpy(validation_X).to(device), torch.from_numpy(validation_y).to(device))\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "# torch dataloaders (shuffle data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xyd-_H53sQ63",
    "outputId": "5ef3c544-98a8-494f-96c6-1370eb4b52a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Input size:  torch.Size([32, 512])\n",
      "Example Input:\n",
      " tensor([[    0,     0,     0,  ...,    20, 14267,   678],\n",
      "        [    0,     0,     0,  ...,  2287,  1934, 43333],\n",
      "        [ 6475,    43,     3,  ...,     3,  2749,  1111],\n",
      "        ...,\n",
      "        [   28,     4,     1,  ...,     4,  1479, 21055],\n",
      "        [    0,     0,     0,  ...,   485,    98,  2346],\n",
      "        [    0,     0,     0,  ...,     4,   169,  1258]], device='cuda:0')\n",
      "\n",
      "Example Output size:  torch.Size([32])\n",
      "Example Output:\n",
      " tensor([0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1.],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# get a batch of train data\n",
    "train_data_iter = iter(train_dataloader)\n",
    "X_example, y_example = train_data_iter.next()\n",
    "print('Example Input size: ', X_example.size()) # batch_size, seq_length\n",
    "print('Example Input:\\n', X_example)\n",
    "print()\n",
    "print('Example Output size: ', y_example.size()) # batch_size\n",
    "print('Example Output:\\n', y_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScCGusrDsQ63"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dimension, embedding_dimension, hidden_dimension, output_dimension):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(input_dimension, embedding_dimension)  \n",
    "        self.rnn_layer = nn.RNN(embedding_dimension, hidden_dimension, num_layers=1)\n",
    "        self.fc_layer = nn.Linear(hidden_dimension, output_dimension)\n",
    "        \n",
    "    def forward(self, sequence):\n",
    "        # sequence shape = (sequence_length, batch_size)\n",
    "        embedding = self.embedding_layer(sequence)  \n",
    "        # embedding shape = [sequence_length, batch_size, embedding_dimension]\n",
    "        output, hidden_state = self.rnn_layer(embedding)\n",
    "        # output shape = [sequence_length, batch_size, hidden_dimension]\n",
    "        # hidden_state shape = [1, batch_size, hidden_dimension]\n",
    "        final_output = self.fc_layer(hidden_state[-1,:,:].squeeze(0))      \n",
    "        return final_output\n",
    "    \n",
    "input_dimension = len(vocab_to_token)+1 # +1 to account for padding\n",
    "embedding_dimension = 100\n",
    "hidden_dimension = 32\n",
    "output_dimension = 1\n",
    "\n",
    "rnn_model = RNN(input_dimension, embedding_dimension, hidden_dimension, output_dimension)\n",
    "\n",
    "optim = torch.optim.Adam(rnn_model.parameters())\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "rnn_model = rnn_model.to(device)\n",
    "loss_func = loss_func.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83WOqdB7sQ64"
   },
   "outputs": [],
   "source": [
    "def accuracy_metric(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Returns 0-1 accuracy for the given set of predictions and ground truth\n",
    "    \"\"\"\n",
    "    # round predictions to either 0 or 1\n",
    "    rounded_predictions = torch.round(torch.sigmoid(predictions))\n",
    "    success = (rounded_predictions == ground_truth).float() #convert into float for division \n",
    "    accuracy = success.sum() / len(success)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fozNkV0gsQ64"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optim, loss_func):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    model.train()\n",
    "    \n",
    "    for sequence, sentiment in dataloader:\n",
    "        optim.zero_grad()     \n",
    "        preds = model(sequence.T).squeeze()\n",
    "        \n",
    "        loss_curr = loss_func(preds, sentiment)\n",
    "        accuracy_curr = accuracy_metric(preds, sentiment)\n",
    "        \n",
    "        loss_curr.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        loss += loss_curr.item()\n",
    "        accuracy += accuracy_curr.item()\n",
    "        \n",
    "    return loss/len(dataloader), accuracy/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4wv7AaisQ65"
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, loss_func):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequence, sentiment in dataloader:\n",
    "            \n",
    "            preds = model(sequence.T).squeeze()\n",
    "            \n",
    "            loss_curr = loss_func(preds, sentiment)   \n",
    "            accuracy_curr = accuracy_metric(preds, sentiment)\n",
    "\n",
    "            loss += loss_curr.item()\n",
    "            accuracy += accuracy_curr.item()\n",
    "        \n",
    "    return loss/len(dataloader), accuracy/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AuGRwLOHsQ65",
    "outputId": "96a48916-7ce0-4d74-f619-8ac123fc8469",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 1 | time elapsed: 14.243398427963257s\n",
      "training loss: 0.619 | training accuracy: 66.86%\n",
      "validation loss: 0.899 |  validation accuracy: 36.22%\n",
      "\n",
      "epoch number: 2 | time elapsed: 14.03232741355896s\n",
      "training loss: 0.528 | training accuracy: 74.46%\n",
      "validation loss: 0.989 |  validation accuracy: 34.98%\n",
      "\n",
      "epoch number: 3 | time elapsed: 14.05145525932312s\n",
      "training loss: 0.433 | training accuracy: 80.65%\n",
      "validation loss: 0.814 |  validation accuracy: 58.15%\n",
      "\n",
      "epoch number: 4 | time elapsed: 14.11315107345581s\n",
      "training loss: 0.346 | training accuracy: 85.77%\n",
      "validation loss: 0.790 |  validation accuracy: 62.11%\n",
      "\n",
      "epoch number: 5 | time elapsed: 14.052831411361694s\n",
      "training loss: 0.309 | training accuracy: 87.44%\n",
      "validation loss: 1.016 |  validation accuracy: 58.05%\n",
      "\n",
      "epoch number: 6 | time elapsed: 14.01461410522461s\n",
      "training loss: 0.259 | training accuracy: 89.98%\n",
      "validation loss: 1.005 |  validation accuracy: 59.78%\n",
      "\n",
      "epoch number: 7 | time elapsed: 14.282076835632324s\n",
      "training loss: 0.239 | training accuracy: 91.26%\n",
      "validation loss: 0.946 |  validation accuracy: 63.33%\n",
      "\n",
      "epoch number: 8 | time elapsed: 14.045422077178955s\n",
      "training loss: 0.188 | training accuracy: 93.32%\n",
      "validation loss: 1.120 |  validation accuracy: 62.02%\n",
      "\n",
      "epoch number: 9 | time elapsed: 14.044484615325928s\n",
      "training loss: 0.145 | training accuracy: 95.10%\n",
      "validation loss: 0.989 |  validation accuracy: 67.23%\n",
      "\n",
      "epoch number: 10 | time elapsed: 14.009809017181396s\n",
      "training loss: 0.148 | training accuracy: 95.01%\n",
      "validation loss: 0.772 |  validation accuracy: 75.25%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "best_validation_loss = float('inf')\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "\n",
    "    time_start = time.time()\n",
    "    \n",
    "    training_loss, train_accuracy = train(rnn_model, train_dataloader, optim, loss_func)\n",
    "    validation_loss, validation_accuracy = validate(rnn_model, validation_dataloader, loss_func)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_delta = time_end - time_start  \n",
    "    \n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        torch.save(rnn_model.state_dict(), 'rnn_model.pt')\n",
    "    \n",
    "    print(f'epoch number: {ep+1} | time elapsed: {time_delta}s')\n",
    "    print(f'training loss: {training_loss:.3f} | training accuracy: {train_accuracy*100:.2f}%')\n",
    "    print(f'validation loss: {validation_loss:.3f} |  validation accuracy: {validation_accuracy*100:.2f}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6sTgrtSsQ65"
   },
   "outputs": [],
   "source": [
    "def sentiment_inference(model, sentence):\n",
    "    model.eval()\n",
    "    \n",
    "    # text transformations\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join([c for c in sentence if c not in punctuation])\n",
    "    tokenized = [vocab_to_token.get(token, 0) for token in sentence.split()]\n",
    "    tokenized = np.pad(tokenized, (512-len(tokenized), 0), 'constant')\n",
    "    \n",
    "    # model inference\n",
    "    model_input = torch.LongTensor(tokenized).to(device)\n",
    "    model_input = model_input.unsqueeze(1)\n",
    "    pred = torch.sigmoid(model(model_input))\n",
    "    \n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DV11N1AqsQ65",
    "outputId": "2ea8b240-0048-487f-a277-006db77f1607"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004703204613178968\n",
      "0.005312444642186165\n",
      "0.10777481645345688\n",
      "0.7247500419616699\n",
      "0.2506076395511627\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_inference(rnn_model, \"This film is horrible\"))\n",
    "print(sentiment_inference(rnn_model, \"Director tried too hard but this film is bad\"))\n",
    "print(sentiment_inference(rnn_model, \"Decent movie, although could be shorter\"))\n",
    "print(sentiment_inference(rnn_model, \"This film will be houseful for weeks\"))\n",
    "print(sentiment_inference(rnn_model, \"I loved the movie, every part of it\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Z3n4JcNwkOl",
    "outputId": "826a79d5-75e7-42cb-a406-ed04744a3295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6335980892181396\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_inference(rnn_model, \"I liked the movie a lot\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EAI6010_Movie_sentiments_rnn__Tutorial_week3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
